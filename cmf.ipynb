{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# %% Dataset Class\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, json_path, root_dir, id_range, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.start_id, self.end_id = id_range\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.data, self.entry_count = self._prepare_data(data)\n",
    "\n",
    "    def _prepare_data(self, data):\n",
    "        prepared_data = []\n",
    "        entry_count = 0\n",
    "\n",
    "        for patient_id, patient_data in data.items():\n",
    "            if self.start_id <= patient_id <= self.end_id:\n",
    "                for side in ['Right', 'Left', 'Right1', 'Left1', 'Right2', 'Left2', 'Right3', 'Left3']:\n",
    "                    if side in patient_data:\n",
    "                        side_data = patient_data[side]\n",
    "                        label = side_data['Label']\n",
    "                        image_paths = side_data.get(\"Paths\", {})\n",
    "\n",
    "                        images = []\n",
    "                        skip_entry = False\n",
    "\n",
    "                        for img_type in [\"deep\", \"surface\"]:\n",
    "                            if img_type in image_paths:\n",
    "                                path = image_paths[img_type]\n",
    "                                full_path = os.path.abspath(os.path.join(self.root_dir, path))\n",
    "                                if os.path.exists(full_path):\n",
    "                                    images.append(full_path)\n",
    "                                else:\n",
    "                                    skip_entry = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                skip_entry = True\n",
    "                                break\n",
    "\n",
    "                        if skip_entry:\n",
    "                            continue\n",
    "\n",
    "                        prepared_data.append({\n",
    "                            'patient_id': patient_id,\n",
    "                            'side': side,\n",
    "                            'images': images,\n",
    "                            'label': label\n",
    "                        })\n",
    "                        entry_count += 1\n",
    "\n",
    "        return prepared_data, entry_count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     item = self.data[idx]\n",
    "     images = []\n",
    "    \n",
    "     for img_path in item['images']:\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Convert to grayscale (1 channel)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        images.append(img)\n",
    "    \n",
    "     if len(images) != 2:\n",
    "        raise ValueError(f\"Expected 2 images per entry, but got {len(images)} for patient {item['patient_id']} side {item['side']}\")\n",
    "    \n",
    "    # Stack the two images along the channel dimension to create a 2-channel tensor\n",
    "     images = torch.stack(images)  # Shape: [2, H, W]\n",
    "     label_tensor = torch.tensor(item['label'], dtype=torch.float32)\n",
    "\n",
    "     return images, label_tensor, item['patient_id'], item['side']\n",
    "\n",
    "\n",
    "\n",
    "# %% Define paths, ID range, and transforms\n",
    "json_path = \"train.json\"\n",
    "root_dir = \"\"\n",
    "id_range = (\"20230402140053\", \"20230708145810\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = PatientDataset(json_path, root_dir, id_range, transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "# %% Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Cases per Unique Label:\n",
      "Unique Label 1: 3\n",
      "Unique Label 0_3: 22\n",
      "Unique Label 0_1: 36\n",
      "Unique Label : 151\n",
      "Unique Label 0: 31\n",
      "Unique Label 0_1_3: 27\n",
      "Unique Label 3: 21\n",
      "Unique Label 0_3_4: 3\n",
      "Unique Label 2_3: 4\n",
      "Unique Label 1_2_3: 6\n",
      "Unique Label 0_1_3_4: 4\n",
      "Unique Label 0_4: 5\n",
      "Unique Label 1_2: 11\n",
      "Unique Label 4: 4\n",
      "Unique Label 3_4: 3\n",
      "Unique Label 0_1_4: 4\n",
      "Unique Label 2: 1\n",
      "\n",
      "Unique Label Distribution (%):\n",
      "Unique Label 1: 0.89%\n",
      "Unique Label 0_3: 6.55%\n",
      "Unique Label 0_1: 10.71%\n",
      "Unique Label : 44.94%\n",
      "Unique Label 0: 9.23%\n",
      "Unique Label 0_1_3: 8.04%\n",
      "Unique Label 3: 6.25%\n",
      "Unique Label 0_3_4: 0.89%\n",
      "Unique Label 2_3: 1.19%\n",
      "Unique Label 1_2_3: 1.79%\n",
      "Unique Label 0_1_3_4: 1.19%\n",
      "Unique Label 0_4: 1.49%\n",
      "Unique Label 1_2: 3.27%\n",
      "Unique Label 4: 1.19%\n",
      "Unique Label 3_4: 0.89%\n",
      "Unique Label 0_1_4: 1.19%\n",
      "Unique Label 2: 0.30%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a counter to count unique labels\n",
    "label_counter = Counter()\n",
    "\n",
    "# Loop through the entire dataset to count unique labels\n",
    "for _, label, _, _ in dataset:\n",
    "    # Convert one-hot encoded label to a unique string identifier\n",
    "    active_indices = [str(i) for i, value in enumerate(label) if value == 1]\n",
    "    unique_label = \"_\".join(active_indices)  # Join active indices with \"_\"\n",
    "    label_counter[unique_label] += 1\n",
    "\n",
    "# Compute total samples and distribution\n",
    "total_samples = len(dataset)\n",
    "distribution = {label: count / total_samples * 100 for label, count in label_counter.items()}\n",
    "\n",
    "# Display results\n",
    "print(\"Total Number of Cases per Unique Label:\")\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"Unique Label {label}: {count}\")\n",
    "\n",
    "print(\"\\nUnique Label Distribution (%):\")\n",
    "for label, dist in distribution.items():\n",
    "    print(f\"Unique Label {label}: {dist:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 2. Apply SMOTE to balance the dataset\u001b[39;00m\n\u001b[0;32m     24\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m features_resampled, labels_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Check the distribution of resampled labels\u001b[39;00m\n\u001b[0;32m     28\u001b[0m unique_labels, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels_resampled, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     97\u001b[0m check_classification_targets(y)\n\u001b[0;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\base.py:156\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_target_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicate_one_vs_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\utils\\_validation.py:177\u001b[0m, in \u001b[0;36mcheck_target_type\u001b[1;34m(y, indicate_one_vs_all)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_y \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImbalanced-learn currently supports binary, multiclass and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinarized encoded multiclasss targets. Multilabel and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultioutput targets are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         )\n\u001b[0;32m    182\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming PatientDataset is already defined and `json_path`, `root_dir`, `id_range`, `transform` are provided\n",
    "dataset = PatientDataset(json_path, root_dir, id_range, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 1. Extract features and labels from the dataset\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for images, targets, _, _ in dataloader:  # Ensure your dataset's __getitem__ returns correct structure\n",
    "    images = images.view(images.size(0), -1)  # Flatten the images for SMOTE\n",
    "    features.append(images.cpu().numpy())  # Convert to numpy\n",
    "    labels.append(targets.cpu().numpy())  # Convert to numpy\n",
    "\n",
    "# Concatenate all batches\n",
    "features = np.vstack(features)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# 2. Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "features_resampled, labels_resampled = smote.fit_resample(features, labels)\n",
    "\n",
    "# Check the distribution of resampled labels\n",
    "unique_labels, counts = np.unique(labels_resampled, return_counts=True)\n",
    "print(\"Resampled Label Distribution:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label}: {count}\")\n",
    "\n",
    "# 3. Reshape the resampled features back to original image dimensions\n",
    "# Assuming the original images have dimensions [channels, height, width] (e.g., [3, 224, 224])\n",
    "channels, height, width = 3, 224, 224\n",
    "features_resampled = features_resampled.reshape(-1, channels, height, width)\n",
    "\n",
    "# Convert resampled features and labels to tensors\n",
    "features_resampled_tensor = torch.tensor(features_resampled, dtype=torch.float32)\n",
    "labels_resampled_tensor = torch.tensor(labels_resampled, dtype=torch.long)\n",
    "\n",
    "# 4. Create a new Dataset class for resampled data\n",
    "class ResampledPatientDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create a dataloader for the resampled dataset\n",
    "resampled_dataset = ResampledPatientDataset(features_resampled_tensor, labels_resampled_tensor)\n",
    "resampled_dataloader = DataLoader(resampled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Now, `resampled_dataloader` can be used for training your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Cases per Disease:\n",
      "Disease 1: 91\n",
      "Disease 0: 132\n",
      "Disease 3: 90\n",
      "Disease 4: 23\n",
      "Disease 2: 22\n",
      "\n",
      "Disease Distribution (%):\n",
      "Disease 1: 27.08%\n",
      "Disease 0: 39.29%\n",
      "Disease 3: 26.79%\n",
      "Disease 4: 6.85%\n",
      "Disease 2: 6.55%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Initialize a counter for disease occurrences\n",
    "disease_counter = Counter()\n",
    "\n",
    "# Loop through the dataset to count diseases\n",
    "for _, label_tensor, _, _ in dataset:\n",
    "    # If the label is a one-hot vector, convert it to indices\n",
    "    if label_tensor.ndim > 0:\n",
    "        diseases = torch.nonzero(label_tensor).squeeze().tolist()\n",
    "        if isinstance(diseases, int):  # Handle single disease case\n",
    "            diseases = [diseases]\n",
    "        for disease in diseases:\n",
    "            disease_counter[disease] += 1\n",
    "    else:  # Handle case where labels are single integers\n",
    "        disease_counter[int(label_tensor.item())] += 1\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# Compute disease distribution\n",
    "disease_distribution = {disease: count / total_samples * 100 for disease, count in disease_counter.items()}\n",
    "\n",
    "# Display results\n",
    "print(\"Total Number of Cases per Disease:\")\n",
    "for disease, count in disease_counter.items():\n",
    "    print(f\"Disease {disease}: {count}\")\n",
    "\n",
    "print(\"\\nDisease Distribution (%):\")\n",
    "for disease, dist in disease_distribution.items():\n",
    "    print(f\"Disease {disease}: {dist:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\manoj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\manoj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\manoj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\manoj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\manoj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for x,i,_,_ in train_loader:\n",
    "    print(x.shape)\n",
    "     \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, num_modalities=2, num_patches=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_modalities = num_modalities\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # Linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Positional and modality embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches + 1), embed_dim))\n",
    "        self.modality_embeddings = nn.Parameter(torch.randn(num_modalities, 1, embed_dim))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # MLP block\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, z0):\n",
    "        \"\"\"\n",
    "        Input: z0 - Tensor of shape [(num_modalities * num_patches + 1), embed_dim]\n",
    "        \"\"\"\n",
    "        B, _, D = z0.shape  # Batch size, tokens, embedding dimension\n",
    "        \n",
    "        # Linear projection with positional and modality embeddings\n",
    "        z_proj = self.projection(z0)\n",
    "        z_proj += self.pos_embedding[:, :z0.shape[1], :]\n",
    "        z_proj += torch.cat(\n",
    "            [self.modality_embeddings[i].repeat(1, z_proj.shape[1] // self.num_modalities, 1) for i in range(self.num_modalities)],\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        # Split into queries, keys, and values\n",
    "        queries = z_proj[:, :self.num_patches + 1, :]  # CLS and patches of the first modality\n",
    "        keys = z_proj.reshape(B, -1, D)  # All modalities and patches combined\n",
    "        values = keys.clone()\n",
    "        \n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.multihead_attn(queries, keys, values)\n",
    "        z_proj = z_proj + attn_output\n",
    "        \n",
    "        # Layer norm and MLP with residual connection\n",
    "        z_proj = z_proj + self.mlp(self.norm2(z_proj))\n",
    "        \n",
    "        return z_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalOCT(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use ResNet18 as encoder\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Cross Modal Fusion block\n",
    "        self.cmf = CrossModalFusion(embed_dim=embed_dim)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim, 5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, M, C, H, W = x.shape  # [B, 2, 3, 224, 224]\n",
    "        \n",
    "        # Reshape for encoder\n",
    "        x = x.view(B * M, C, H, W)\n",
    "        \n",
    "        # Encode each image\n",
    "        features = self.encoder(x)\n",
    "        features = features.squeeze(-1).squeeze(-1)\n",
    "        features = features.view(B, M, -1)\n",
    "        \n",
    "        # Apply cross-modal fusion\n",
    "        fused_features = self.cmf(features)\n",
    "        \n",
    "        # Flatten for classifier\n",
    "        fused_features = fused_features.reshape(B, -1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused_features)\n",
    "        output = output.squeeze()  # Remove all singular dimensions\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Input images shape: torch.Size([4, 2, 3, 224, 224])\n",
      "Input labels shape: torch.Size([4, 5])\n",
      "Model output shape: torch.Size([4, 5])\n",
      "Epoch 1/5\n",
      "Train Loss: 0.4878, Train Accuracy: 79.55%\n",
      "Val Loss: 0.6185, Val Accuracy: 60.00%\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4549, Train Accuracy: 77.24%\n",
      "Val Loss: 0.5749, Val Accuracy: 80.00%\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4174, Train Accuracy: 82.01%\n",
      "Val Loss: 0.6403, Val Accuracy: 70.00%\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4179, Train Accuracy: 80.97%\n",
      "Val Loss: 0.4040, Val Accuracy: 80.00%\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4406, Train Accuracy: 80.15%\n",
      "Val Loss: 0.5646, Val Accuracy: 80.00%\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Print first batch shapes\n",
    "    for images, labels, patient_ids, sides in train_loader:\n",
    "        print(f\"Input images shape: {images.shape}\")\n",
    "        print(f\"Input labels shape: {labels.shape}\")\n",
    "        outputs = model(images.to(device))\n",
    "        print(f\"Model output shape: {outputs.shape}\")\n",
    "        break\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(len(train_loader)):\n",
    "            images, labels, patient_ids, sides = next(iter(train_loader))\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Check if we need to adjust output shape to match labels\n",
    "            if outputs.shape != labels.shape:\n",
    "                if len(outputs.shape) < len(labels.shape):\n",
    "                    outputs = outputs.unsqueeze(-1)\n",
    "                elif len(labels.shape) < len(outputs.shape):\n",
    "                    labels = labels.unsqueeze(-1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            train_correct += torch.sum(predictions == labels).item()\n",
    "            train_total += labels.numel()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = (train_correct / train_total) * 100\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_loader)):\n",
    "                images, labels, patient_ids, sides = next(iter(val_loader))\n",
    "                images = images.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Match dimensions\n",
    "                if outputs.shape != labels.shape:\n",
    "                    if len(outputs.shape) < len(labels.shape):\n",
    "                        outputs = outputs.unsqueeze(-1)\n",
    "                    elif len(labels.shape) < len(outputs.shape):\n",
    "                        labels = labels.unsqueeze(-1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                val_correct += torch.sum(predictions == labels).item()\n",
    "                val_total += labels.numel()\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = (val_correct / val_total) * 100\n",
    "        \n",
    "        # Learning rate scheduling based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print('-' * 60)\n",
    "\n",
    "# Initialize and train model\n",
    "model = MultiModalOCT()\n",
    "train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, num_modalities=2, num_patches=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_modalities = num_modalities\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # Linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Positional and modality embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches + 1) * num_modalities, embed_dim))\n",
    "        self.modality_embeddings = nn.Parameter(torch.randn(num_modalities, 1, embed_dim))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # MLP block\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, z0):\n",
    "        \"\"\"\n",
    "        Input: z0 - Tensor of shape [batch_size, (num_modalities * num_patches + 1), embed_dim]\n",
    "        \"\"\"\n",
    "        B, T, D = z0.shape  # Batch size, tokens, embedding dimension\n",
    "\n",
    "        # Linear projection with positional embeddings\n",
    "        z_proj = self.projection(z0)\n",
    "        z_proj += self.pos_embedding[:, :T, :]\n",
    "\n",
    "        # Modality-specific embeddings\n",
    "        modality_tokens = T // self.num_modalities\n",
    "        modality_embeds = torch.cat(\n",
    "            [self.modality_embeddings[i].expand(B, modality_tokens, D) for i in range(self.num_modalities)],\n",
    "            dim=1\n",
    "        )\n",
    "        z_proj += modality_embeds\n",
    "\n",
    "        # Split into queries, keys, and values\n",
    "        queries = z_proj[:, :self.num_patches + 1, :]  # CLS and patches of the first modality\n",
    "        keys = z_proj  # All modalities and patches combined\n",
    "        values = keys.clone()\n",
    "\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.multihead_attn(queries, keys, values)\n",
    "        z_proj = z_proj + attn_output\n",
    "\n",
    "        # Layer norm and MLP with residual connection\n",
    "        z_proj = self.norm1(z_proj)\n",
    "        z_proj = z_proj + self.mlp(self.norm2(z_proj))\n",
    "\n",
    "        return z_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
