{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 297\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved new best model with validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 258\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    249\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PatientDataset(\n\u001b[0;32m    250\u001b[0m     json_path\u001b[38;5;241m=\u001b[39mjson_path,\n\u001b[0;32m    251\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mroot_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m    255\u001b[0m )\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Initialize model and training components\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, json_path, root_dir, id_range, modalities, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.start_id, self.end_id = id_range\n",
    "        self.transform = transform\n",
    "        self.modalities = modalities\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.data, self.entry_count = self._prepare_data(data)\n",
    "        \n",
    "    def _prepare_data(self, data):\n",
    "        prepared_data = []\n",
    "        entry_count = 0\n",
    "        \n",
    "        for patient_id in range(self.start_id, self.end_id + 1):\n",
    "            if str(patient_id) in data:\n",
    "                patient_data = data[str(patient_id)]\n",
    "                for side in ['Right', 'Left', 'Right1', 'Left1', 'Right2', 'Left2', 'Right3', 'Left3']:\n",
    "                    if side in patient_data:\n",
    "                        side_data = patient_data[side]\n",
    "                        label = side_data['Label']\n",
    "                        image_paths = side_data.get(\"Paths\", {})\n",
    "                        \n",
    "                        images = []\n",
    "                        skip_entry = False\n",
    "                        \n",
    "                        for img_type in self.modalities:\n",
    "                            if img_type in image_paths:\n",
    "                                path = image_paths[img_type]\n",
    "                                full_path = os.path.join(self.root_dir, path)\n",
    "                                if os.path.exists(full_path):\n",
    "                                    images.append(full_path)\n",
    "                                else:\n",
    "                                    skip_entry = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                skip_entry = True\n",
    "                                break\n",
    "                        \n",
    "                        if skip_entry or len(images) != len(self.modalities):\n",
    "                            continue\n",
    "                        \n",
    "                        prepared_data.append({\n",
    "                            'patient_id': patient_id,\n",
    "                            'side': side,\n",
    "                            'images': images,\n",
    "                            'label': label\n",
    "                        })\n",
    "                        entry_count += 1\n",
    "        \n",
    "        return prepared_data, entry_count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        images = []\n",
    "        \n",
    "        for img_path in item['images']:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        label_tensor = torch.tensor(item['label'], dtype=torch.float32)\n",
    "        \n",
    "        return images, label_tensor, item['patient_id'], item['side']\n",
    "\n",
    "class OCTEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.project = nn.Linear(512, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, num_modalities, num_patches, d_model, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_modalities = num_modalities\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_modalities * num_patches + 1, d_model))\n",
    "        self.modality_embed = nn.Parameter(torch.randn(1, num_modalities, d_model))\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def prepare_inputs(self, features):\n",
    "        B = features.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, features], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        modality_embeddings = self.modality_embed.repeat_interleave(self.num_patches, dim=1)\n",
    "        x[:, 1:] = x[:, 1:] + modality_embeddings.expand(B, -1, -1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.prepare_inputs(features)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        cls_token = x[:, 0]\n",
    "        output = self.classifier(cls_token)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "class OCTMultiModalModel(nn.Module):\n",
    "    def __init__(self, num_modalities, num_patches=1, d_model=256):\n",
    "        super().__init__()\n",
    "        self.encoder = OCTEncoder(d_model)\n",
    "        self.fusion = CrossModalFusion(num_modalities, num_patches, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, M, C, H, W = x.shape\n",
    "        features = []\n",
    "        for i in range(M):\n",
    "            modality_features = self.encoder(x[:, i])\n",
    "            features.append(modality_features.unsqueeze(1))\n",
    "        features = torch.cat(features, dim=1)\n",
    "        output = self.fusion(features)\n",
    "        return output\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels, _, _ in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = (outputs.squeeze() > 0.5).cpu().detach().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1),\n",
    "                                'accuracy': accuracy})\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    epoch_auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy, epoch_auc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels, _, _ in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs.squeeze() > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            accuracy = accuracy_score(all_labels, all_preds)\n",
    "            progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1),\n",
    "                                    'accuracy': accuracy})\n",
    "    \n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_accuracy, val_auc\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    json_path = \"train.json\"\n",
    "    root_dir = \"\"\n",
    "    modalities = [\"modality1\", \"modality2\", \"modality3\"]  # Replace with your modality names\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PatientDataset(\n",
    "        json_path=json_path,\n",
    "        root_dir=root_dir,\n",
    "        id_range=(1, 800),  # Training patient IDs\n",
    "        modalities=modalities,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = PatientDataset(\n",
    "        json_path=json_path,\n",
    "        root_dir=root_dir,\n",
    "        id_range=(801, 1000),  # Validation patient IDs\n",
    "        modalities=modalities,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = OCTMultiModalModel(\n",
    "        num_modalities=len(modalities),\n",
    "        d_model=256\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_accuracy, train_auc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, AUC: {train_auc:.4f}\")\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_accuracy, val_auc = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Saved new best model with validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# %% Dataset Class\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, json_path, root_dir, id_range, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.start_id, self.end_id = id_range\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.data, self.entry_count = self._prepare_data(data)\n",
    "\n",
    "    def _prepare_data(self, data):\n",
    "        prepared_data = []\n",
    "        entry_count = 0\n",
    "\n",
    "        for patient_id, patient_data in data.items():\n",
    "            if self.start_id <= patient_id <= self.end_id:\n",
    "                for side in ['Right', 'Left', 'Right1', 'Left1', 'Right2', 'Left2', 'Right3', 'Left3']:\n",
    "                    if side in patient_data:\n",
    "                        side_data = patient_data[side]\n",
    "                        label = side_data['Label']\n",
    "                        image_paths = side_data.get(\"Paths\", {})\n",
    "\n",
    "                        images = []\n",
    "                        skip_entry = False\n",
    "\n",
    "                        for img_type in [\"deep\", \"surface\"]:\n",
    "                            if img_type in image_paths:\n",
    "                                path = image_paths[img_type]\n",
    "                                full_path = os.path.abspath(os.path.join(self.root_dir, path))\n",
    "                                if os.path.exists(full_path):\n",
    "                                    images.append(full_path)\n",
    "                                else:\n",
    "                                    skip_entry = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                skip_entry = True\n",
    "                                break\n",
    "\n",
    "                        if skip_entry:\n",
    "                            continue\n",
    "\n",
    "                        prepared_data.append({\n",
    "                            'patient_id': patient_id,\n",
    "                            'side': side,\n",
    "                            'images': images,\n",
    "                            'label': label\n",
    "                        })\n",
    "                        entry_count += 1\n",
    "\n",
    "        return prepared_data, entry_count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     item = self.data[idx]\n",
    "     images = []\n",
    "    \n",
    "     for img_path in item['images']:\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Convert to grayscale (1 channel)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        images.append(img)\n",
    "    \n",
    "     if len(images) != 2:\n",
    "        raise ValueError(f\"Expected 2 images per entry, but got {len(images)} for patient {item['patient_id']} side {item['side']}\")\n",
    "    \n",
    "    # Stack the two images along the channel dimension to create a 2-channel tensor\n",
    "     images = torch.stack(images)  # Shape: [2, H, W]\n",
    "     label_tensor = torch.tensor(item['label'], dtype=torch.float32)\n",
    "\n",
    "     return images, label_tensor, item['patient_id'], item['side']\n",
    "\n",
    "\n",
    "\n",
    "# %% Define paths, ID range, and transforms\n",
    "json_path = \"train.json\"\n",
    "root_dir = \"C:\\\\Users\\\\manoj\\\\OneDrive\\\\Desktop\\\\INTERN\\\\train\"\n",
    "id_range = (\"20230402140053\", \"20230708145810\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = PatientDataset(json_path, root_dir, id_range, transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "# %% Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\manoj/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:25<00:00, 1.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16, 5])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 199\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    198\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 199\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msetup_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 181\u001b[0m, in \u001b[0;36msetup_training\u001b[1;34m(train_loader, val_loader)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m     train_loss, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     val_loss, val_f1 \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 121\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m--> 121\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3148\u001b[0m     )\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([16, 5])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "class OCTEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # Modify first conv layer to accept grayscale images\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.project = nn.Linear(512, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, num_modalities=2, d_model=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_modalities = num_modalities\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Positional and modality embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_modalities + 1, d_model))\n",
    "        self.modality_embed = nn.Parameter(torch.randn(1, num_modalities, d_model))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        \n",
    "        # MLP\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def prepare_inputs(self, features):\n",
    "        B = features.shape[0]\n",
    "        \n",
    "        # Expand CLS token for batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        \n",
    "        # Concatenate CLS token with features\n",
    "        x = torch.cat([cls_tokens, features], dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Add modality embeddings to features (not CLS token)\n",
    "        x[:, 1:] = x[:, 1:] + self.modality_embed\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.prepare_inputs(features)\n",
    "        \n",
    "        # Self-attention block\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = x + residual\n",
    "        \n",
    "        # MLP block\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        # Use CLS token for classification\n",
    "        cls_token = x[:, 0]\n",
    "        output = self.classifier(cls_token)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "class OCTMultiModalModel(nn.Module):\n",
    "    def __init__(self, d_model=256):\n",
    "        super().__init__()\n",
    "        self.encoder = OCTEncoder(d_model)\n",
    "        self.fusion = CrossModalFusion(num_modalities=2, d_model=d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, M, C, H, W = x.shape  # Batch, Modalities (2), Channels, Height, Width\n",
    "        \n",
    "        # Encode each modality\n",
    "        features = []\n",
    "        for i in range(M):\n",
    "            modality_features = self.encoder(x[:, i])  # [B, d_model]\n",
    "            features.append(modality_features.unsqueeze(1))  # Add modality dimension\n",
    "            \n",
    "        # Concatenate features from both modalities\n",
    "        features = torch.cat(features, dim=1)  # [B, 2, d_model]\n",
    "        \n",
    "        # Pass through fusion module\n",
    "        output = self.fusion(features)\n",
    "        return output\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels, _, _) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        preds = (outputs.squeeze() > 0.5).cpu().detach().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, f1\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs.squeeze() > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, f1\n",
    "\n",
    "# Training setup code\n",
    "def setup_training(train_loader, val_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = OCTMultiModalModel(d_model=256).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Saved new best model with validation F1: {val_f1:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Start training\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = setup_training(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16, 5])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 174\u001b[0m\n\u001b[0;32m    172\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msetup_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 155\u001b[0m, in \u001b[0;36msetup_training\u001b[1;34m(train_loader, val_loader)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m     train_loss, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     val_loss, val_f1 \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 97\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 97\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Now both will be [B]\u001b[39;00m\n\u001b[0;32m     99\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    100\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3148\u001b[0m     )\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([16, 5])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "class OCTEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.project = nn.Linear(512, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, num_modalities=2, d_model=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_modalities = num_modalities\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_modalities + 1, d_model))\n",
    "        self.modality_embed = nn.Parameter(torch.randn(1, num_modalities, d_model))\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def prepare_inputs(self, features):\n",
    "        B = features.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, features], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x[:, 1:] = x[:, 1:] + self.modality_embed\n",
    "        return x\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.prepare_inputs(features)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        cls_token = x[:, 0]\n",
    "        output = self.classifier(cls_token)\n",
    "        return output  # Shape: [B, 1]\n",
    "\n",
    "class OCTMultiModalModel(nn.Module):\n",
    "    def __init__(self, d_model=256):\n",
    "        super().__init__()\n",
    "        self.encoder = OCTEncoder(d_model)\n",
    "        self.fusion = CrossModalFusion(num_modalities=2, d_model=d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, M, C, H, W = x.shape\n",
    "        \n",
    "        features = []\n",
    "        for i in range(M):\n",
    "            modality_features = self.encoder(x[:, i])\n",
    "            features.append(modality_features.unsqueeze(1))\n",
    "            \n",
    "        features = torch.cat(features, dim=1)\n",
    "        output = self.fusion(features)\n",
    "        return output  # Shape: [B, 1]\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels, _, _) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Now both will be [B]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        preds = (outputs.squeeze() > 0.5).cpu().detach().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, f1\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)  # Now both will be [B]\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs.squeeze() > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, f1\n",
    "\n",
    "def setup_training(train_loader, val_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = OCTMultiModalModel(d_model=256).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Saved new best model with validation F1: {val_f1:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Start training\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = setup_training(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Checking training data:\n",
      "\n",
      "Checking data shapes...\n",
      "Batch 0:\n",
      "Images shape: torch.Size([16, 2, 3, 224, 224])\n",
      "Labels shape: torch.Size([16, 5])\n",
      "Sample patient ID: 20230424165605\n",
      "Sample side: Right\n",
      "\n",
      "Checking validation data:\n",
      "\n",
      "Checking data shapes...\n",
      "Batch 0:\n",
      "Images shape: torch.Size([16, 2, 3, 224, 224])\n",
      "Labels shape: torch.Size([16, 5])\n",
      "Sample patient ID: 20230531162152\n",
      "Sample side: Left\n",
      "Using device: cuda\n",
      "Train dataset size: 126\n",
      "Val dataset size: 32\n",
      "Sample batch images shape: torch.Size([16, 2, 3, 224, 224])\n",
      "Sample batch labels shape: torch.Size([16, 5])\n",
      "\n",
      "Epoch 1/50\n",
      "Images shape: torch.Size([16, 2, 3, 224, 224])\n",
      "Labels shape: torch.Size([16, 5])\n",
      "Raw outputs shape: torch.Size([16, 1])\n",
      "Squeezed outputs shape: torch.Size([16])\n",
      "Final outputs shape: torch.Size([16])\n",
      "Final labels shape: torch.Size([80])\n",
      "Error during training: Target size (torch.Size([80])) must be the same as input size (torch.Size([16]))\n",
      "Stack trace:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_32892\\2077344965.py\", line 100, in setup_training\n",
      "    train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_32892\\2077344965.py\", line 28, in train_epoch\n",
      "    loss = criterion(outputs, labels)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 731, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\manoj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3224, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n",
      "ValueError: Target size (torch.Size([80])) must be the same as input size (torch.Size([16]))\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels, _, _) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Print output shape\n",
    "        print(f\"Raw outputs shape: {outputs.shape}\")\n",
    "        print(f\"Squeezed outputs shape: {outputs.squeeze().shape}\")\n",
    "        \n",
    "        # Ensure labels are the right shape\n",
    "        labels = labels.view(-1)  # Reshape to [B]\n",
    "        outputs = outputs.view(-1)  # Reshape to [B]\n",
    "        \n",
    "        print(f\"Final outputs shape: {outputs.shape}\")\n",
    "        print(f\"Final labels shape: {labels.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        preds = (outputs > 0.5).cpu().detach().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "        # Break after first batch during debugging\n",
    "        break\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, f1\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            labels = labels.view(-1)  # Reshape to [B]\n",
    "            outputs = model(images).view(-1)  # Reshape to [B]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, f1\n",
    "\n",
    "def setup_training(train_loader, val_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Val dataset size: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    # Get a sample batch\n",
    "    sample_images, sample_labels, _, _ = next(iter(train_loader))\n",
    "    print(f\"Sample batch images shape: {sample_images.shape}\")\n",
    "    print(f\"Sample batch labels shape: {sample_labels.shape}\")\n",
    "    \n",
    "    model = OCTMultiModalModel(d_model=256).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Changed from BCELoss to handle raw logits\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            \n",
    "            train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "            \n",
    "            scheduler.step(val_f1)\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(f'Saved new best model with validation F1: {val_f1:.4f}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        print(\"Stack trace:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Data shape checking function\n",
    "def check_data_shapes(loader):\n",
    "    print(\"\\nChecking data shapes...\")\n",
    "    for batch_idx, (images, labels, patient_ids, sides) in enumerate(loader):\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Sample patient ID: {patient_ids[0]}\")\n",
    "        print(f\"Sample side: {sides[0]}\")\n",
    "        break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check data shapes before training\n",
    "    print(\"\\nChecking training data:\")\n",
    "    check_data_shapes(train_loader)\n",
    "    print(\"\\nChecking validation data:\")\n",
    "    check_data_shapes(val_loader)\n",
    "    \n",
    "    model = setup_training(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
